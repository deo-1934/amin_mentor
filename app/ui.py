# app/ui.py
import os
from pathlib import Path
import textwrap

import streamlit as st
from dotenv import load_dotenv

from settings import UIConfig, DEFAULT_TOP_K, DEFAULT_EMBED_MODEL
from retriever import Retriever

# ---- Bootstrap ----
BASE_DIR = Path(__file__).resolve().parents[1]
load_dotenv(BASE_DIR / ".env")

st.set_page_config(page_title="Amin Mentor", page_icon="ğŸ§ ", layout="wide")
cfg = UIConfig()

# ---- Sidebar ----
st.sidebar.title("âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª")
mode = st.sidebar.radio("Ø­Ø§Ù„Øª Ù¾Ø§Ø³Ø®", ["Ø¢ÙÙ„Ø§ÛŒÙ† (Ø¨Ø¯ÙˆÙ† LLM)", "Ø¢Ù†Ù„Ø§ÛŒÙ† (Ø¨Ø§ LLM)"], index=0)
top_k = st.sidebar.slider("ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ (k)", 1, 15, DEFAULT_TOP_K, 1)
embed_model_name = st.sidebar.text_input("Ù…Ø¯Ù„ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯", value=DEFAULT_EMBED_MODEL)
openai_key = st.sidebar.text_input("OpenAI API Key (Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª Ø¢Ù†Ù„Ø§ÛŒÙ†)", type="password")

st.sidebar.caption("Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø§ÛŒØ¯ Ø¯Ø± `faiss_index/` Ø­Ø§Ø¶Ø± Ø¨Ø§Ø´Ø¯ (index.faiss + meta.json).")

# ---- Title ----
st.title(cfg.title)
st.write(cfg.description)

# ---- Load retriever ----
@st.cache_resource(show_spinner=True)
def _get_retriever(name: str) -> Retriever:
    return Retriever(embed_model=name)

try:
    retriever = _get_retriever(embed_model_name)
except Exception as e:
    st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³/Ù…Ø¯Ù„: {e}")
    st.stop()

# ---- Query input ----
col_q1, col_q2 = st.columns([4, 1])
with col_q1:
    query = st.text_input("â“ Ù¾Ø±Ø³Ø´ Ø´Ù…Ø§", placeholder="Ù…Ø«Ù„Ø§Ù‹: Ø¨Ù‡ØªØ±ÛŒÙ† Ø´ÛŒÙˆÙ‡ Ù…Ø°Ø§Ú©Ø±Ù‡ Ø¯Ø± Ø´Ø±Ø§ÛŒØ· ÙØ´Ø§Ø± Ø²Ù…Ø§Ù†ÛŒ Ú†ÛŒØ³ØªØŸ")
with col_q2:
    ask = st.button("Ø¬Ø³Øªâ€ŒÙˆØ¬Ùˆ", use_container_width=True)

# ---- Helper: Offline summarizer ----
def offline_answer(query: str, hits):
    # Ù¾Ø§Ø³Ø® Ø®ÛŒÙ„ÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚Ø·Ø¹Ø§Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒâ€ŒØ´Ø¯Ù‡ (Ø¨Ø¯ÙˆÙ† LLM)
    if not hits:
        return "Ù…ÙˆØ±Ø¯ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."
    joined = "\n\n".join([f"- {h['text']}" for h in hits])
    answer = textwrap.shorten(joined, width=900, placeholder=" ...")
    return f"**Ø®Ù„Ø§ØµÙ‡â€ŒÛŒ Ø³Ø±ÛŒØ¹ Ø§Ø² Ù…ØªÙˆÙ† Ù…Ø±ØªØ¨Ø·:**\n\n{answer}"

# ---- Main action ----
if ask and query:
    with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÛŒ Ø§Ø³Ù†Ø§Ø¯ Ù…Ø±ØªØ¨Ø·..."):
        hits = retriever.search(query, k=top_k)

    if not hits:
        st.warning("Ù‡ÛŒÚ† Ø³Ù†Ø¯ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ù¾ÙˆØ´Ù‡ `data/` Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù† Ùˆ Ø§Ú¯Ø± Ø®Ø§Ù„ÛŒ Ø§Ø³ØªØŒ ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù† Ùˆ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø³Ø§Ø².")
    else:
        col_ans, col_refs = st.columns([2, 1], gap="large")

        # Answer panel
        with col_ans:
            st.subheader("ğŸ§© Ù¾Ø§Ø³Ø®")
            if mode.startswith("Ø¢ÙÙ„Ø§ÛŒÙ†"):
                st.markdown(offline_answer(query, hits))
            else:
                if not openai_key:
                    st.info("Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª Ø¢Ù†Ù„Ø§ÛŒÙ†ØŒ Ú©Ù„ÛŒØ¯ OpenAI Ø±Ø§ Ø¯Ø± Ø³Ø§ÛŒØ¯Ø¨Ø§Ø± ÙˆØ§Ø±Ø¯ Ú©Ù†. ÙØ¹Ù„Ø§Ù‹ Ø­Ø§Ù„Øª Ø¢ÙÙ„Ø§ÛŒÙ† Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
                    st.markdown(offline_answer(query, hits))
                else:
                    # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ ÙØ±Ø§Ø®ÙˆØ§Ù† LLM Ø®ÙˆØ¯Øª Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒ (ÙØ¹Ù„Ø§Ù‹ Ø³Ø§Ø¯Ù‡ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ…)
                    st.markdown(offline_answer(query, hits))
                    st.caption("ğŸ’¡ Ø§ØªØµØ§Ù„ LLM Ù‚Ø§Ø¨Ù„ Ø§ÙØ²ÙˆØ¯Ù† Ø§Ø³Øª (ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ØªØ§ÛŒØ¬ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒâ€ŒØ´Ø¯Ù‡).")

        # References panel
        with col_refs:
            st.subheader("ğŸ“ Ù…Ù†Ø§Ø¨Ø¹")
            for i, h in enumerate(hits, 1):
                with st.expander(f"{i}. {h.get('source','Ù†Ø§Ù…Ø´Ø®Øµ')}  â€¢  Ø§Ù…ØªÛŒØ§Ø²: {h['score']:.3f}  â€¢  chunk: {h.get('chunk_idx','-')}"):
                    st.write(h.get("text", "â€”"))
                    meta_line = []
                    if h.get("path"): meta_line.append(f"`{h['path']}`")
                    if h.get("source"): meta_line.append(f"source: **{h['source']}**")
                    if meta_line:
                        st.caption(" | ".join(meta_line))
