"""
test_search.py
---------------
ØªØ³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ… Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ FAISS.
"""

import pickle
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from pathlib import Path


# Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
ROOT = Path(__file__).resolve().parents[1]
INDEX_PATH = ROOT / "faiss_index" / "index.faiss"
STORE_PATH = ROOT / "faiss_index" / "store.pkl"

# Ù…Ø¯Ù„ Embedding Ù…ÙˆØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"


def load_index_and_data():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ."""
    print("ğŸ“¦ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
    index = faiss.read_index(str(INDEX_PATH))
    with open(STORE_PATH, "rb") as f:
        store = pickle.load(f)
    return index, store


def semantic_search(query: str, top_k: int = 3):
    """Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ù¾Ø±Ø³Ø´ Ø¯Ø± Ù…ÛŒØ§Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§."""
    index, store = load_index_and_data()
    model = SentenceTransformer(MODEL_NAME)

    print("ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒÛŒ embedding Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø³Ø´...")
    q_embed = model.encode([query], normalize_embeddings=True)
    q_embed = np.asarray(q_embed, dtype="float32")

    print("ğŸ” Ø¬Ø³Øªâ€ŒÙˆØ¬Ùˆ Ø¯Ø± Ø§ÛŒÙ†Ø¯Ú©Ø³...")
    distances, indices = index.search(q_embed, top_k)

    print("\nâœ… Ù†ØªØ§ÛŒØ¬ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ø¨Ø®Ø´â€ŒÙ‡Ø§:\n")
    for rank, (idx, dist) in enumerate(zip(indices[0], distances[0])):
        doc = store["docs"][idx]
        meta = store["meta"][idx]
        print(f"{rank+1}. ({meta['source']} - chunk {meta['chunk_idx']})")
        print(f"ÙØ§ØµÙ„Ù‡: {dist:.4f}")
        print(f"Ù…ØªÙ†: {doc[:200]}...")
        print("-" * 80)


if __name__ == "__main__":
    q = input("â“ Ù¾Ø±Ø³Ø´ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: ")
    semantic_search(q)
