#FEYZ
#DEO
# -*- coding: utf-8 -*-
"""
generator.py - Cost-Tiered LLM Routing

ูุฏู:
- ููุดู ุงุฒ ูุฏู ููุด ูุตููุน ุงุณุชูุงุฏู ฺฉู (ุฏฺฏู ุฌูุงุจ ุจุฏูู ูุฏู ููโุฏู)
- ูู ุจุณุชู ุจู ุณุฎุช ุณุคุงูุ ูุฏู ุงุฑุฒููโุชุฑ ุง ูุฏู ููโุชุฑ ุฑู ุตุฏุง ุจุฒู
- ุจุฑุง ุณูุงูโูุง ุณุงุฏู ุชูฺฉู ฺฉูุชุฑ ุฎุฑุฌ ฺฉู
- ุจุฑุง ุณูุงูโูุง ุฌุฏ ุงุฒ ูุฏู ูู ุจุง ุชูฺฉู ุจุดุชุฑ ุงุณุชูุงุฏู ฺฉู

ูุฑูุฏ ุงุตู ุงุฒ ui/web ูโุขุฏ:
    generate_answer(query=user_text, context=full_context, ...)

context ุชุฑฺฉุจู ุงุฒ:
- ุชฺฉูโูุง ุฏุงูุด ุฏุงุฎู (retriever)
- ุฎูุงุตู ถ ูพุงู ุขุฎุฑ ูฺฉุงููู ("ฺฏูุชฺฏู ุชุง ุงูุฌุง: ...")

ุฎุฑูุฌ ุจุงุฏ ฺฉ ูุชู ูุญุงูุฑูโุง ู ฺฉโุชฺฉู ุจุงุดู.
"""

import os, json, re
from pathlib import Path
from typing import Dict, Any, Optional, List

try:
    # OpenAI SDK ุฌุฏุฏ
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None


# -------------------------------------------------
# ุฎูุงูุฏู ุชูุธูุงุช ู ุณฺฉุฑุชโูุง
# -------------------------------------------------
def _read_secret_or_env(key: str, default: str = "") -> str:
    """
    ุงูู ุชูุงุด ูโฺฉูู ุงุฒ st.secrets ุจุฎููู (Streamlit Cloud)
    ุจุนุฏ ูุฑู ุณุฑุงุบ os.environ (ููฺฉุงู .env)
    """
    try:
        import streamlit as st  # type: ignore
        if key in getattr(st, "secrets", {}):
            return str(st.secrets.get(key, default))
    except Exception:
        pass
    return os.getenv(key, default)


def load_settings() -> Dict[str, Any]:
    """
    ุจุฑูโฺฏุฑุฏููู ุชูุธูุงุช runtime ุดุงูู ูุฏูโูุงุ api key ู ูุณุฑ ฺฉุด.
    """
    base_dir = Path(__file__).resolve().parents[1]
    data_dir = base_dir / "data"
    data_dir.mkdir(parents=True, exist_ok=True)
    cache_path = data_dir / "cache.json"

    return {
        "MODEL_PROVIDER": _read_secret_or_env("MODEL_PROVIDER", "openai").strip().lower(),

        # ูุฏู ุงุฑุฒูู ุจุฑุง ุณูุงูโูุง ฺฉูุชุงู/ุฑูุชู/ุณุงุฏู
        "OPENAI_MODEL_CHEAP": _read_secret_or_env("OPENAI_MODEL_CHEAP", "gpt-4o-mini"),

        # ูุฏู ููโุชุฑ ุจุฑุง ุณูุงูโูุง ุฌุฏ ุจุฒูุณ/ุงุณุชุฑุงุชฺ
        "OPENAI_MODEL_DEEP": _read_secret_or_env("OPENAI_MODEL_DEEP", "gpt-4o-mini"),

        "OPENAI_API_KEY": _read_secret_or_env("OPENAI_API_KEY", ""),

        # ุจุฑุง ุณุงุฒฺฏุงุฑ ุจุง ูุณุฎูโูุง ูุจู (HuggingFace ู ...)ุ ุงูุฌุง ููุท ูฺฏู ุฏุงุดุชู ุดุฏู:
        "HF_TOKEN": _read_secret_or_env("HF_TOKEN", ""),

        # ูุณุฑ ฺฉุด ุจุฑุง ฺฉุงูุด ูุฒูู ุณุคุงูุงุช ุชฺฉุฑุงุฑ
        "CACHE_PATH": str(cache_path),
    }


def _load_cache(path: str) -> Dict[str, Any]:
    try:
        p = Path(path)
        if p.exists():
            return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        pass
    return {}


def _save_cache(path: str, data: Dict[str, Any]) -> None:
    try:
        Path(path).write_text(
            json.dumps(data, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
    except Exception:
        pass


# -------------------------------------------------
# ุชุดุฎุต ููุน ุณูุงู (ุงุฑุฒูู ุง ฺฏุฑููุ)
# -------------------------------------------------
def _is_smalltalk_or_simple(query: str) -> bool:
    """
    ุงฺฏุฑ ุณุคุงู ุฎู ฺฉูุชุงูู ุง ุตุฑูุงู ุงุญูุงูโูพุฑุณ/ุฎูุงุณุชู ุณุงุฏู ุงุณุชุ
    ูโุชููู ุงุฒ ูุฏู ุงุฑุฒููโุชุฑ ู ุชูฺฉู ฺฉู ุงุณุชูุงุฏู ฺฉูู.
    """
    q = (query or "").strip().lower()

    greetings = [
        "ุณูุงู", "ุณูุงู.", "ุณูุงู!",
        "hi", "hello", "hey",
        "ุฎูุจ", "ฺุทูุฑ", "ฺู ุฎุจุฑ", "ุฎุณุชู ูุจุงุด",
        "ุตุจุญ ุจุฎุฑ", "ุดุจ ุจุฎุฑ",
    ]

    # ุงฺฏุฑ ุฌููู ุฎู ฺฉูุชุงู ู ุดุจู ุงุญูุงูโูพุฑุณ ุจูุฏ
    if len(q) <= 25:
        for g in greetings:
            if g in q:
                return True

    # ุณูุงูโูุง ุฎู ูุฑููู ู ูุณุชูู ู ฺฉูุชุงู:
    # "ุงุตูู ูุฐุงฺฉุฑู ุฑู ูุงู ุจุจุฑ"
    # "ุชุนุฑู ุชูุฑฺฉุฒ ฺูุ"
    # "ฺูุฏ ุชุง ูฺฉุชู ุจฺฏู"
    patterns_simple = [
        r"^ุงุตูู",          # ุงุตูู ูุฐุงฺฉุฑู ุฑู ุจฺฏู
        r"^ุชุนุฑู",         # ุชุนุฑู ุชูุฑฺฉุฒ ฺูุ
        r"^ุนู ฺ",       # ุจูุฑูโูุฑ ุนู ฺ
        r"^ฺูุฏ ุชุง ูฺฉุชู",   # ฺูุฏ ุชุง ูฺฉุชู ุจฺฏู
    ]
    for pat in patterns_simple:
        if re.search(pat, q):
            return True

    # ุงฺฏุฑ ุฎู ฺฉูุชุงูู ู ููุท ู ุฏุณุชูุฑ ฺฉูฺฺฉู:
    # "ู ูฺฉุชู ุฏุฑ ููุฑุฏ ูุฐุงฺฉุฑู ุจฺฏู"
    if len(q.split()) <= 6:
        return True

    # ุฏุฑ ุบุฑ ุงู ุตูุฑุช ูโุชููู ูพฺุฏูโุชุฑ ุจุงุดู
    return False


def _clean_context_blocks(context_list: Optional[List[str]]) -> str:
    """
    ูุฑูุฏ context (ูุณุช ุชฺฉูโูุง ุฏุงูุด/ฺฏูุชฺฏู) ุฑู ุชูุฒ ู ฺฉ ูโฺฉูู
    ุชุง ูุฏู ุฑุงุญุชโุชุฑ ุงุฒุด ุงุณุชูุงุฏู ฺฉูู.
    """
    if not context_list:
        return ""

    cleaned_blocks: List[str] = []
    for block in context_list:
        if not block:
            continue
        # ูพุงฺฉ ฺฉุฑุฏู ฺุฒูุง ฺฉู ูุจุงุฏ ูุณุชููุงู ฺฉุงุฑุจุฑ ุจุจูุฏ
        txt = re.sub(r"\(ููุจุน:[^)]+\)", "", block)
        txt = re.sub(r"\[[^\]]+\]", "", txt)
        txt = txt.strip()
        if txt:
            cleaned_blocks.append(txt)

    if not cleaned_blocks:
        return ""

    merged = "\n\n---\n\n".join(cleaned_blocks)
    final = (
        "ุงุฏุฏุงุดุช ฺฉูฺฉ (ุณุงุจูู ฺฏูุชฺฏู ู ุฏุงูุด ุฏุงุฎู):\n"
        "ุงุฒ ุงู ุงุทูุงุนุงุช ููุท ุจุฑุง ุงูฺฉู ุจูุชุฑ ู ุฏููโุชุฑ ุฌูุงุจ ุจุฏ ุงุณุชูุงุฏู ฺฉู. "
        "ุงู ูุชู ุฑู ูุณุชูู ุชฺฉุฑุงุฑ ูฺฉู ูฺฏุฑ ูุงุฒู ุจุงุดุฏ.\n\n"
        f"{merged}\n"
    )
    return final


#DEO
# -------------------------------------------------
# ุชูุงุณ ุจุง OpenAI
# -------------------------------------------------
def _call_openai(
    *,
    api_key: str,
    model_name: str,
    prompt: str,
    max_tokens: int,
    temperature: float,
) -> str:
    """
    ุตุฏุง ุฒุฏู OpenAI Responses API.
    ูุง ุงูุชุธุงุฑ ุฏุงุฑู ูุฏูโูุง ูุซู gpt-4o / gpt-4o-mini ุงู ุฑุง ุณุงูพูุฑุช ฺฉููุฏ.
    ุฎุฑูุฌ ุฑุง ุจู ฺฉ ูุชู ุชูุฒ ุชุจุฏู ูโฺฉูู.
    """
    if not OpenAI:
        raise RuntimeError("openai package not available in this environment")

    client = OpenAI(api_key=api_key)

    response = client.responses.create(
        model=model_name,
        input=prompt,
        max_output_tokens=max_tokens,
        temperature=temperature,
    )

    # ุงุณุชุฎุฑุงุฌ ูุชู ุงุฒ ุณุงุฎุชุงุฑ response
    text_out = None

    try:
        if hasattr(response, "output") and response.output:
            parts = []
            for item in response.output:
                # item ููฺฉูู .content (ูุณุช segmentูุง) ุง .text ุฏุงุดุชู ุจุงุดู
                if hasattr(item, "content") and item.content:
                    segs = []
                    for seg in item.content:
                        if hasattr(seg, "text") and seg.text:
                            segs.append(seg.text)
                    if segs:
                        parts.append("\n".join(segs))
                elif hasattr(item, "text") and item.text:
                    parts.append(item.text)
            if parts:
                text_out = "\n".join(parts).strip()
    except Exception:
        pass

    # fallbackูุง ุงุญุชูุงู
    if text_out is None and hasattr(response, "output_text"):
        try:
            text_out = str(response.output_text).strip()
        except Exception:
            pass

    if text_out is None and hasattr(response, "choices"):
        try:
            if response.choices and hasattr(response.choices[0], "message"):
                msg = response.choices[0].message
                if hasattr(msg, "content"):
                    text_out = str(msg.content).strip()
        except Exception:
            pass

    if text_out is None:
        text_out = str(response)

    return text_out


# -------------------------------------------------
# ุชุงุจุน ุงุตู ูพุงุณุฎโุฏู
# -------------------------------------------------
def generate_answer(
    query: str,
    *,
    context: Optional[List[str]] = None,
    temperature_simple: float = 0.2,
    temperature_deep: float = 0.3,
    max_tokens_simple: int = 128,
    max_tokens_deep: int = 512,
    force_new: bool = False,   # ๐ ุฌุฏุฏ: ุงฺฏุฑ True ุจุงุดุฏุ ฺฉุด ุฑุง ูุงุฏุฏู ูโฺฏุฑู
) -> str:
    """
    ููุดู ูุฏู ุฑู ุตุฏุง ูโุฒูู.
    ูู:
      - ุงฺฏุฑ ุณูุงู ุณุงุฏูโุณุช โ ูุฏู ุงุฑุฒููโุชุฑุ ุชูฺฉู ฺฉู
      - ุงฺฏุฑ ุณูุงู ุฌุฏโุชุฑู โ ูุฏู ููโุชุฑุ ุชูฺฉู ุจุดุชุฑ
      - ุงฺฏุฑ force_new == True โ ฺฉุด ุฑุง ูุงุฏุฏู ูโฺฏุฑู ู ุญุช ุงฺฏุฑ ุงู ุณุคุงู ุชฺฉุฑุงุฑ ุงุณุชุ ุฌูุงุจ ุฌุฏุฏ ูโฺฏุฑู

    ุฎุฑูุฌ: ฺฉ ูุชู ูุญุงูุฑูโุงุ ฺฉโุชฺฉูุ ุจุฏูู ุณุฑูุตูโูุง ุฎุดฺฉ.
    """

    s = load_settings()

    provider = s["MODEL_PROVIDER"]
    api_key = s["OPENAI_API_KEY"]
    model_cheap = s["OPENAI_MODEL_CHEAP"]
    model_deep = s["OPENAI_MODEL_DEEP"]

    cache_path = s["CACHE_PATH"]
    cache = _load_cache(cache_path)

    # context ุฑู ุชูุฒ ฺฉูู
    ctx_block = _clean_context_blocks(context)

    # ฺฉูุฏ ฺฉุด: ุณุคุงู ฺฉุงุฑุจุฑ + ฺฉุงูุชฺฉุณุช
    cache_key = f"{query.strip()}##{ctx_block.strip()}"

    # ุงฺฏุฑ force_new=False ู ุงู ุณุคุงู ูุจูุง ุฌูุงุจ ุฏุงุฏู ุดุฏูุ ููุงู ูพุงุณุฎ ุฑุง ุจุฏู
    if (not force_new) and cache_key in cache:
        return cache[cache_key]

    # ุชุตูู ุจฺฏุฑู ฺฉู ุงู ุณูุงู "ุณุงุฏู/ฺฉูุชุงู" ุงุณุช ุง "ุฌุฏ/ุนูู"
    simple = _is_smalltalk_or_simple(query)

    if simple:
        chosen_model = model_cheap
        chosen_temp = temperature_simple
        chosen_max_tokens = max_tokens_simple
        style_instruction = (
            "ุฎู ุฎูุงุตู ู ุฎูุฏูุงู ุฌูุงุจ ุจุฏู. "
            "ฺฉ ูพุงุฑุงฺฏุฑุงู ุง ฺูุฏ ุฌููู ฺฉูุชุงู ฺฉุงูู. "
            "ุฒุงุฏ ุชุฆูุฑฺฉ ู ุฏุงูุดฺฏุงู ูุจุงุด. "
            "ูุงุถุญ ู ูุณุชูู ุจุงุด."
        )
    else:
        chosen_model = model_deep
        chosen_temp = temperature_deep
        chosen_max_tokens = max_tokens_deep
        style_instruction = (
            "ูุซู ฺฉ ููุชูุฑ ฺฉุณุจโูฺฉุงุฑ ูุงุฑุณ ุฑูุชุงุฑ ฺฉู. "
            "ุฌูุงุจ ุฑู ฺฉุงุฑุจุฑุฏุ ูุดุฎุต ู ูุฑุญููโุง ุจุฏูุ ูู ุฎุดฺฉ ู ุฑุณู ูุจุงุด. "
            "ุฎุฑูุฌ ููุท ฺฉ ูุชู ฺฉโุชฺฉู ุจุงุดูุ ุณุฑูุตูู ุฑุณู ู ุชุชุฑ ูุฐุงุฑ."
        )

    # ูพุฑุงููพุช ููุง ฺฉู ูุฏู ุจุงุฏ ุจฺฏุฑู
    prompt = (
        f"{style_instruction}\n\n"
        f"ุณูุงู ฺฉุงุฑุจุฑ:\n{query.strip()}\n\n"
        f"{ctx_block}"
    )

    # enforce ุงูฺฉู ูุนูุงู ููุท openai ุณุงูพูุฑุช ูโุดู
    if provider != "openai":
        provider = "openai"

    # ุฏุฑุฎูุงุณุช ุจู LLM
    if provider == "openai" and api_key:
        try:
            answer_text = _call_openai(
                api_key=api_key,
                model_name=chosen_model,
                prompt=prompt,
                max_tokens=chosen_max_tokens,
                temperature=chosen_temp,
            )
        except Exception:
            answer_text = (
                "ุงูุงู ูุชููุณุชู ุฌูุงุจ ููุดููุฏ ุฑู ุงุฒ ูุฏู ุจฺฏุฑู. "
                "ู ุจุงุฑ ุฏฺฏู ุจูพุฑุณ ุง ูุงุถุญโุชุฑ ุจฺฏู ุฏููุง ุฏูุจุงู ฺ ูุณุช."
            )
    else:
        answer_text = (
            "ุฏุฑ ุญุงู ุญุงุถุฑ ุจู ูุฏู ูุชุตู ูุณุชู. ฺฉูุฏ API ุง ุณุทุญ ุฏุณุชุฑุณ ููุฌูุฏ ูุณุช."
        )

    # ูพุงุณุฎ ุฌุฏุฏ ุฑู ุฏุฑ ฺฉุด ุฐุฎุฑู ฺฉู
    cache[cache_key] = answer_text
    _save_cache(cache_path, cache)

    return answer_text

#FEYZ
#DEO
