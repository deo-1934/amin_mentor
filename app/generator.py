#FEYZ
#DEO
# -*- coding: utf-8 -*-
"""
generator.py (ูุณุฎู Human+SmartCost Routing)

ููุทู ูพุงุณุฎโุฏู:
1) ุงุญูุงูโูพุฑุณ/ูพุงู ุณุงุฏู โ ุฌูุงุจ ฺฉูุชุงูุ ฺฏุฑูุ ุงูุณุงู. ุจุฏูู ูุฏูุ ุจุฏูู ูุฒูู.
2) Rule-based โ ุงฺฏุฑ ุณุคุงู ุชฺฉุฑุงุฑ/ูุงุถุญ ุจุงุดู ุงุฒ ุฌูุงุจโูุง ุงุฒูพุดโุขูุงุฏู ุงุณุชูุงุฏู ูโุดู. ุจุฏูู ูุฒูู.
3) Retrieval โ ุงฺฏุฑ ุชู ุฏุงูุด ุฏุงุฎู ูุง (ฺฉุชุงุจ/ุงุฏุฏุงุดุชโูุง) ุฌูุงุจ ุฑูุดู ุจุงุดูุ ุงุฒ ูููู ุงุณุชูุงุฏู ูโุดู. ุจุฏูู ูุฒูู API.
4) OpenAI / HuggingFace โ ููุท ุงฺฏุฑ ูููุฒ ุฌูุงุจ ุฎูุจ ูุฏุงุฑู ูโุฑู ุณุฑุงุบ ูุฏู ุงุจุฑ.
5) fallback โ ุงฺฏุฑ ููู ฺุฒ ูุทุน ุดุฏุ ฺฉ ุชูุตูโ ุงุฌุฑุง ฺฉูุชุงู ู ุงูุณุงู ูโุฏู.

ุฎุฑูุฌ ููุดู ฺฉ ูุชู ฺฉโุชฺฉู ุงุณุช.
"""

import os, json, random, requests
from pathlib import Path
from typing import Dict, Any, Optional, List

# ุชูุงุด ุจุฑุง ุงููพูุฑุช OpenAI SDK ุฌุฏุฏ
try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None


def _read_secret_or_env(key: str, default: str = "") -> str:
    """
    ุงูู ุงุฒ st.secrets (ุฑู Streamlit Cloud)
    ุจุนุฏ ุงุฒ os.environ (ููฺฉุงู .env)
    """
    try:
        import streamlit as st  # type: ignore
        if key in getattr(st, "secrets", {}):
            return str(st.secrets.get(key, default))
    except Exception:
        pass
    return os.getenv(key, default)


def load_settings() -> Dict[str, Any]:
    """
    ููุงุฏุฑ ุงุชุตุงู ุจู ูุฏูโูุง + ูุณุฑ ฺฉุด
    """
    base_dir = Path(__file__).resolve().parents[1]
    data_dir = base_dir / "data"
    data_dir.mkdir(parents=True, exist_ok=True)
    cache_path = data_dir / "cache.json"

    return {
        "MODEL_PROVIDER": _read_secret_or_env("MODEL_PROVIDER", "openai").strip().lower(),
        "OPENAI_API_KEY": _read_secret_or_env("OPENAI_API_KEY", ""),
        "OPENAI_MODEL": _read_secret_or_env("OPENAI_MODEL", "gpt-4o-mini"),
        "MODEL_ENDPOINT": _read_secret_or_env(
            "MODEL_ENDPOINT",
            "https://api-inference.huggingface.co/models/gpt2"
        ),
        "HF_TOKEN": _read_secret_or_env("HF_TOKEN", ""),
        "CACHE_PATH": str(cache_path),
        "DEFAULT_MAX_NEW_TOKENS": int(_read_secret_or_env("MAX_NEW_TOKENS", "256")),
        "DEFAULT_TEMPERATURE": float(_read_secret_or_env("TEMPERATURE", "0.2")),
    }


def _load_cache(path: str) -> Dict[str, Any]:
    try:
        p = Path(path)
        if p.exists():
            return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        pass
    return {}


def _save_cache(path: str, data: Dict[str, Any]) -> None:
    try:
        Path(path).write_text(
            json.dumps(data, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
    except Exception:
        pass


def _join_context(context: Optional[List[str]]) -> str:
    """
    context = ุชฺฉูโูุง ฺฉู retriever ุจุฑฺฏุฑุฏููุฏู (ุฏุงูุด ููฺฉุงู ูุง)
    ูุง ุงูู ููุท ุจู ูุฏู ุงุจุฑ ูโุฏู ุชุง ฺฉูฺฉุด ฺฉูู ุฏููโุชุฑ ุจุงุดู.
    ูุณุชูู ุจู ฺฉุงุฑุจุฑ ูุดูู ุฏุงุฏู ููโุดู.
    """
    if not context:
        return ""
    cleaned = [c.strip() for c in context if c and c.strip()]
    if not cleaned:
        return ""
    ctx = "\n\n".join(cleaned)
    return (
        "\n\n[ุงุฏุฏุงุดุช ุฏุงุฎู ุจุฑุง ูุฏู: ุงุฒ ุงู ุฏุชุง ุจูโุนููุงู ูุฑุฌุน ุงุณุชูุงุฏู ฺฉู. ุงู ูุชู ูุณุชูู ุจู ฺฉุงุฑุจุฑ ููุงุด ุฏุงุฏู ููโุดูุฏ]\n"
        f"{ctx}\n"
    )

#DEO
# ุฌูุงุจโูุง ุขูุงุฏู ุจุฑุง ุณูุงูโูุง ูพุฑุชฺฉุฑุงุฑ (ุฑุงฺฏุงูุ ุจุฏูู API)
_RULES = {
    "streamlit": (
        "ุจุฑุง ุงูฺฉู ูพุฑูฺูโุช ุฑู Streamlit Cloud ุจุฏูู ุฎุทุง ุจุงูุง ุจุงุฏุ ูุณุฎูู ูพุงุชูู ุฑู ุฏุฑ runtime.txt ููู ฺฉู "
        "ูุซูุงู 3.11ุ ูุชุบุฑูุง ุญุณุงุณ ุฑู ุจุฐุงุฑ ุชู Secrets ูู .envุ ู ุฏุงุฎู ูพูุดู app ฺฉ ูุงู ุฎุงู __init__.py "
        "ุจุณุงุฒ ุชุง import ุฎุฑุงุจ ูุดู."
    ),
    "faiss": (
        "ุจุฑุง ุจุงุฒุงุจ ุณุฑุน ุฏุงูุด ูุญู: ูุชูโูุงุฑู ุชูุฒ ู ูพุงุฑุงฺฏุฑุงูโุจูุฏ ฺฉูุ "
        "ุจุง ูุฏู all-MiniLM-L6-v2 ุงูุจุฏูฺฏ ุจฺฏุฑ ู ููููๅุจุฑุฏุงุฑูุง ุฑู ุฏุงุฎู FAISS ุฐุฎุฑู ฺฉู. "
        "ุงูุทูุฑ top_k ุฎู ุณุฑุน ู ุงุฑุฒุงู ุจุฑูโฺฏุฑุฏู."
    ),
    "api": (
        "ุจุฑุง ฺฉู ฺฉุฑุฏู ูุฒููู APIุ ุณูุงูโูุง ุชฺฉุฑุงุฑ ุฑู ฺฉุด ฺฉู ู ููุท ููุช ุณูุงู ูพฺุฏู ุง ุฌุฏุฏ ุดุฏ ุจุฑู ุณุฑุงุบ ูุฏู ุงุจุฑ. "
        "ูุจู ุงุฒ ูุฒูู ฺฉุฑุฏูุ ูุณุฆูู ุฑู ุจู ฺฉ ูุฏู ูุงุจูโุงุฌุฑุง ุฏุฑ ููู ุงูุฑูุฒ ุชุจุฏู ฺฉู."
    ),
    "ูุฐุงฺฉุฑู": (
        "ุชู ูุฐุงฺฉุฑู ุงูู ฺฏูุด ุจุฏู ู ุฏูู ุจููู ุทุฑู ููุงุจู ฺ ูโุฎูุงุฏ. "
        "ูุฏูุช ูุชูุงุนุฏ ฺฉุฑุฏู ุฒูุฑฺฉ ูุณุชุ ูุฏูุช ูพุฏุง ฺฉุฑุฏู ููุทูโุงู ฺฉู ูุฑุฏู ุทุฑู ุญุณ ูฺฉูู ุจุงุฒูุฏูโุงู."
    ),
    "ฺฉุณุจ ู ฺฉุงุฑ": (
        "ูุณุชูู ูุฑ ฺฉุณุจโูฺฉุงุฑ ูููู ุงูู ฺฉู ฺฉ ุฏุฑุฏ ูุงูุน ุฑู ุญู ฺฉููุ ูู ุงูฺฉู ููุท ู ุงุฏู ูุดูฺฏ ุฏุงุดุชู ุจุงุดู. "
        "ุงุฑุฒุด ูุงูุน ุนู ฺุฒ ฺฉู ุทุฑู ููุงุจู ุญุงุถุฑ ุจุงุดู ุจุฑุงุด ูพูู ุง ุฒูุงู ุจุฏู."
    ),
}


def _rule_based_answer(query: str) -> Optional[str]:
    """
    ุงฺฏุฑ ุณูุงู ุดุงูู ฺฉ ุงุฒ ฺฉูุฏูุงฺูโูุง _RULES ุจุงุดู ููููโุฌุง ุฌูุงุจ ูโุฏู.
    ูฺ ูุฒููโุง ูู ูุฏุงุฑู.
    """
    q_low = (query or "").lower()
    for key, val in _RULES.items():
        if key.lower() in q_low:
            return val
    return None


def _is_smalltalk(query: str) -> bool:
    """
    ุชุดุฎุต ุงุญูุงูโูพุฑุณ / ุณูุงู ุณุงุฏู.
    ุงฺฏุฑ ุจูู: ุฌูุงุจ ุฏูุณุชุงูู ู ุฎู ฺฉูุชุงู ุจุฏู (ูู ูุญู ูุนููุ ูู ูุญู ููุงูู)
    """
    low_q = (query or "").strip().lower()
    greetings = [
        "ุณูุงู", "ุณูุงู.", "ุณูุงู!", "ุณูุงููู", "salam",
        "hi", "hello", "hey", "heyy", "helo",
        "ุฎูุจ", "ฺุทูุฑ", "ฺู ุฎุจุฑ", "ุฎุณุชู ูุจุงุด",
        "ุตุจุญ ุจุฎุฑ", "ุดุจ ุจุฎุฑ", "ุฏุฑูุฏ"
    ]

    # ุฎู ฺฉูุชุงู + ฺฉ ุงุฒ ูุงฺูโูุง ุณูุงู/ุงุญูุงูโูพุฑุณ
    if len(low_q) <= 20:
        for g in greetings:
            if g in low_q:
                return True
    return False


def _smalltalk_answer() -> str:
    """
    ุฌูุงุจ ุงูุณุงู ฺฉูุชุงู ุจุฑุง ูพุงูโูุง ุฎู ุณุงุฏู
    ุจุฏูู ูฺ ุงุทูุงุนุงุช ุณูฺฏู ุจุฒูุณ
    """
    candidates = [
        "ุณูุงู ๐ ูู ุงูุฌุงู. ฺ ุชู ุฐููุช ูุณุชุ",
        "ุณูุงู ๐ ุจฺฏู ุจุจูู ุงูุฑูุฒ ุฏุฑฺฏุฑ ฺ ูุณุชุ",
        "ุฏุฑูุฏ ๐ฑ ุขูุงุฏูโุงู ูุฑฺ ุชู ูฺฉุฑุช ูุณุช ุจุดููโู.",
        "ุณูุงู ุฎูุด ุงููุฏ ๐ ุดุฑูุน ฺฉููุ",
    ]
    return random.choice(candidates)


def _hf_generate(
    prompt: str,
    endpoint: str,
    token: str,
    temperature: float,
    max_new_tokens: int,
    timeout: float = 40.0
) -> str:
    """
    ุชูุงุณ ุจุง HuggingFace (ูุฏู ุงุจุฑ ุงุฑุฒููโุชุฑ ุง ุฑุงฺฏุงูโุชุฑ).
    """
    headers = {"Authorization": f"Bearer {token}"} if token else {}
    payload = {
        "inputs": prompt,
        "parameters": {
            "temperature": float(temperature),
            "max_new_tokens": int(max_new_tokens),
            "return_full_text": False,
        },
    }
    r = requests.post(endpoint, headers=headers, json=payload, timeout=timeout)
    r.raise_for_status()
    data = r.json()

    # ุญุงูุชโูุง ุฑุงุฌ ูพุงุณุฎโูุง HF
    if isinstance(data, list) and data and isinstance(data[0], dict) and "generated_text" in data[0]:
        return str(data[0]["generated_text"]).strip()
    if isinstance(data, dict) and "generated_text" in data:
        return str(data["generated_text"]).strip()

    return json.dumps(data, ensure_ascii=False)


def _openai_generate(
    prompt: str,
    api_key: str,
    model_name: str,
    temperature: float,
    max_new_tokens: int
) -> str:
    """
    ุชูุงุณ ุจุง OpenAI (ฺฏุฑุงูโุชุฑู ูุฑุญูู).
    ุงุฒ SDK ุฌุฏุฏ OpenAI ุงุณุชูุงุฏู ูโฺฉูู (client.responses.create).
    """
    if not OpenAI:
        raise RuntimeError("openai package not available in this environment")

    client = OpenAI(api_key=api_key)

    response = client.responses.create(
        model=model_name,
        input=prompt,
        temperature=float(temperature),
        max_output_tokens=int(max_new_tokens),
    )

    text_out = None

    # ูุฏู ุฌุฏุฏ ููฺฉู ุงุณุช ูพุงุณุฎ ุฑุง ุฏุฑ response.output ุจุฑฺฏุฑุฏุงูุฏ
    try:
        if hasattr(response, "output") and response.output:
            parts = []
            for item in response.output:
                # item ููฺฉูู .content (ูุณุช ูุทุนูโูุง) ุง .text ุฏุงุดุชู ุจุงุดู
                if hasattr(item, "content") and item.content:
                    segs = []
                    for seg in item.content:
                        if hasattr(seg, "text") and seg.text:
                            segs.append(seg.text)
                    if segs:
                        parts.append("\n".join(segs))
                elif hasattr(item, "text") and item.text:
                    parts.append(item.text)
            if parts:
                text_out = "\n".join(parts).strip()
    except Exception:
        pass

    # ูุณุฑ fallback
    if (text_out is None) and hasattr(response, "output_text"):
        try:
            text_out = str(response.output_text).strip()
        except Exception:
            pass

    if (text_out is None) and hasattr(response, "choices"):
        try:
            if response.choices and hasattr(response.choices[0], "message"):
                msg = response.choices[0].message
                if hasattr(msg, "content"):
                    text_out = str(msg.content).strip()
        except Exception:
            pass

    if text_out is None:
        text_out = str(response)

    return text_out


def healthcheck() -> Dict[str, Any]:
    """
    ุจุฑุง ุงุณุชูุงุฏู ุฏุงุฎู (ูุงฺฏุ ุฏุจุงฺฏ ูุญู).
    ุฏุฑ UI ููุง ููุงุด ุฏุงุฏู ููโุดู.
    """
    s = load_settings()
    return {
        "provider": s["MODEL_PROVIDER"],
        "has_openai_key": bool(s["OPENAI_API_KEY"]),
        "openai_model": s["OPENAI_MODEL"],
        "has_hf_token": bool(s["HF_TOKEN"]),
    }


def generate_answer(
    query: str,
    *,
    model_provider: Optional[str] = None,
    openai_model: Optional[str] = None,
    openai_api_key: Optional[str] = None,
    model_endpoint: Optional[str] = None,
    hf_token: Optional[str] = None,
    temperature: Optional[float] = None,
    max_new_tokens: Optional[int] = None,
    top_k: int = 5,
    context: Optional[List[str]] = None,
) -> str:
    """
    ุงู ุชุงุจุน ููููู ฺฉู ui.py ุตุฏุง ูโุฒูู.
    ุฎุฑูุฌ: ููุดู ฺฉ ูุชู ฺฉโุชฺฉู ุงูุณุงู.
    """

    s = load_settings()

    provider = (model_provider or s["MODEL_PROVIDER"]).lower()
    temperature = float(temperature if temperature is not None else s["DEFAULT_TEMPERATURE"])
    max_new_tokens = int(max_new_tokens if max_new_tokens is not None else s["DEFAULT_MAX_NEW_TOKENS"])

    api_key = openai_api_key or s["OPENAI_API_KEY"]
    model_name = openai_model or s["OPENAI_MODEL"]
    endpoint = model_endpoint or s["MODEL_ENDPOINT"]
    hf_tok = hf_token or s["HF_TOKEN"]

    cache_path = s["CACHE_PATH"]
    cache = _load_cache(cache_path)

    ctx_list = context or []
    ctx_joined = "\n\n".join(ctx_list)

    # ฺฉุด ุจุฑุง ุณูุงูโูุง ุชฺฉุฑุงุฑ
    cache_key = f"{provider}:{hash((query, ctx_joined, max_new_tokens, temperature))}"
    if cache_key in cache:
        return str(cache[cache_key])

    # 0) ุงฺฏุฑ ููุท ุณูุงู/ุงุญูุงูโูพุฑุณ ุจูุฏ โ ุฌูุงุจ ฺฉูุชุงู ุฑูฺฉุณุ ุจุฏูู ูุฒูู
    if _is_smalltalk(query):
        final_text = _smalltalk_answer()
        cache[cache_key] = final_text
        _save_cache(cache_path, cache)
        return final_text

    # 1) Rule-based (ุณูุงูโูุง ูุงุถุญ ู ูพุฑุชฺฉุฑุงุฑ)
    rb = _rule_based_answer(query)
    if rb:
        final_text = rb
        cache[cache_key] = final_text
        _save_cache(cache_path, cache)
        return final_text

    # 2) Retrieval ูุญู (ุณูุงู ููููู ุงูุง ุฌูุงุจุด ุชู ุฏุชุง ุฎูุฏูููู)
    # ุงฺฏุฑ retriever ุจูููู ุชฺฉู ูุชู ุฏุงุฏู ู ุงูู ุชฺฉู ูุนูโุฏุงุฑูุ ุงุฒ ูููู ุฌูุงุจ ูโุณุงุฒู โ ุจุฏูู ูุฒูู API
    if ctx_list:
        best_snippet = ctx_list[0].strip()
        if len(best_snippet) > 40:
            local_answer = (
                f"{best_snippet}\n\n"
                "ุงฺฏุฑ ูโุฎูุง ุงูู ุชุจุฏู ฺฉูู ุจู ฺฉ ูุฏู ุนูู ุจุฑุง ููู ุงูุฑูุฒุ ุจฺฏู ุฏููุง ุงูุงู ฺฉุฌุง ู ฺ ูโุฎูุง ุงูุฌุงู ุจุดู."
            )
            final_text = local_answer
            cache[cache_key] = final_text
            _save_cache(cache_path, cache)
            return final_text

    # 3) ูุฏู ุงุจุฑ (OpenAI โ ฺฏุฑููุชุฑ / HuggingFace โ ุงุฑุฒููุชุฑ)
    # ููุท ููุช ูุฑุงุญู ุจุงูุง ุฌูุงุจ ฺฉุงู ูุฏุงุฏ

    # 3a) OpenAI
    if provider == "openai" and api_key:
        try:
            prompt = (
                "ุชู ููุด ฺฉ ููุชูุฑ ูุงุฑุณ ุฑู ุฏุงุฑ. ูพุงุณุฎ ุจุงุฏ ุฏูุณุชุงููุ ูุงุจูโุงุฌุฑุง ู ุจุฏูู ุจุฎุดโุจูุฏ ุฑุณู ุจุงุดู. "
                "ุฌูุงุจ ุฑู ฺฉูุชุงู ู ุดูุงู ุจุฏูุ ุงูฺฏุงุฑ ูุณุชูู ุจุง ุทุฑู ุญุฑู ูโุฒู.\n\n"
                f"ุณุคุงู ฺฉุงุฑุจุฑ:\n{query.strip()}\n\n"
                "ุงฺฏุฑ ูุงุฒู ุดุฏ ุงุฒ ุงู ุฏุงูุด ุฏุงุฎู ุงุณุชูุงุฏู ฺฉู ูู ุงุฒ ุญุงูุช ุฎุดฺฉ ู ุฏุงูุดฺฏุงู ุฏูุฑ ฺฉู:\n"
                f"{ctx_joined}\n"
            )
            final_text = _openai_generate(
                prompt=prompt,
                api_key=api_key,
                model_name=model_name,
                temperature=temperature,
                max_new_tokens=max_new_tokens,
            )
            cache[cache_key] = final_text
            _save_cache(cache_path, cache)
            return final_text
        except Exception:
            # ุงฺฏุฑ OpenAI ุดฺฉุณุช ุฎูุฑุฏุ ูโุฑู ุณุฑุงุบ HuggingFace ุง fallback
            pass

    # 3b) HuggingFace
    if provider == "huggingface" and hf_tok:
        try:
            prompt = (
                "ฺฉ ูพุงุณุฎ ฺฉูุชุงูุ ุตูู ู ฺฉุงุฑุจุฑุฏ ุจู ุฒุจุงู ูุงุฑุณ ุจุฏู. ุฑุณู ูุจุงุด.\n\n"
                f"ุณุคุงู:\n{query.strip()}\n\n"
                f"ุฏุงูุด ฺฉูฺฉ:\n{ctx_joined}\n"
            )
            final_text = _hf_generate(
                prompt=prompt,
                endpoint=endpoint,
                token=hf_tok,
                temperature=temperature,
                max_new_tokens=max_new_tokens,
            )
            cache[cache_key] = final_text
            _save_cache(cache_path, cache)
            return final_text
        except Exception:
            pass

    # 4) fallback ููุง (ุงฺฏู ูููโฺ ุงุฒ ฺฉุงุฑ ุงูุชุงุฏ)
    final_text = (
        "ุจุง ูุณุฆููโุงุช ุฑู ุชุจุฏู ฺฉูู ุจู ู ูุฏู ุฎู ฺฉูฺฺฉ ฺฉู ููู ุงูุฑูุฒ ุงูุฌุงูุด ุจุฏ. "
        "ุงูุงู ุฏููุงู ฺฉุฌุง ฺฏุฑ ฺฉุฑุฏุ ููููู ุจฺฏู ุชุง ุจุง ูู ููููโุฌุง ุฑู ุจุงุฒ ฺฉูู."
    )
    cache[cache_key] = final_text
    _save_cache(cache_path, cache)
    return final_text
