# app/generator.py
from __future__ import annotations
import os
import requests
from typing import List, Dict, Any

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Cloud (Ø§Ø² secrets ÛŒØ§ env) ---
MODEL_PROVIDER   = os.getenv("MODEL_PROVIDER", "huggingface")
MODEL_ENDPOINT   = os.getenv("MODEL_ENDPOINT", "https://api-inference.huggingface.co/models/gpt2")
HF_TOKEN         = os.getenv("HF_TOKEN", "")
REQUEST_TIMEOUT  = float(os.getenv("REQUEST_TIMEOUT", "30"))

HEADERS = {}
if MODEL_PROVIDER == "huggingface" and HF_TOKEN:
    HEADERS["Authorization"] = f"Bearer {HF_TOKEN}"


def healthcheck() -> Dict[str, Any]:
    return {
        "provider": MODEL_PROVIDER,
        "has_token": bool(HF_TOKEN),
        "endpoint_set": bool(MODEL_ENDPOINT),
        "timeout_secs": REQUEST_TIMEOUT,
    }


def _call_text_generation_api(prompt: str, max_new_tokens: int = 200) -> str:
    """
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ù„Ø§Ù† Ø¨Ø±Ø§ÛŒ HuggingFace endpoint Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡.
    Ø§Ú¯Ù‡ Ø¨Ø¹Ø¯Ø§ Ø±ÙØªÛŒÙ… Ø±ÙˆÛŒ Ù…Ø¯Ù„ Ø¯ÛŒÚ¯Ù‡ (Ù…ÛŒØ²Ø¨Ø§Ù† Ø®ØµÙˆØµÛŒØŒ Ù„ÙˆÚ©Ø§Ù„ Ùˆ ØºÛŒØ±Ù‡)ØŒ
    ÙÙ‚Ø· Ù‡Ù…ÛŒÙ† ØªØ§Ø¨Ø¹ Ø±Ùˆ Ø¹ÙˆØ¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
    """
    if MODEL_PROVIDER == "huggingface":
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": max_new_tokens,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
            }
        }
        try:
            resp = requests.post(
                MODEL_ENDPOINT,
                headers=HEADERS,
                json=payload,
                timeout=REQUEST_TIMEOUT,
            )
            resp.raise_for_status()
            data = resp.json()
            # HF Ø¨Ø¹Ø¶ÛŒ ÙˆÙ‚ØªØ§ Ù„ÛŒØ³Øª dict Ù…ÛŒØ¯Ù‡ Ø¨Ø§ "generated_text"
            if isinstance(data, list) and len(data) and "generated_text" in data[0]:
                return data[0]["generated_text"]
            # fallback
            return str(data)
        except Exception as e:
            return f"(Ø®Ø·Ø§ÛŒ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ: {e})"

    # fallback Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ provider Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¨Ø§Ø´Ù‡
    return "(Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.)"


def _build_hybrid_prompt(user_question: str,
                         context_chunks: List[str]) -> str:
    """
    Ø§ÛŒÙ†Ø¬Ø§ prompt Ø§ØµÙ„ÛŒ Ù…Ø§ Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒØ´Ù‡.
    Ø§ÛŒÙ† prompt:
    - Ù‡ÙˆÛŒØª Ù…Ù†ØªÙˆØ± Ø§Ù…ÛŒÙ† Ø±Ùˆ ØªØ¹Ø±ÛŒÙ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
    - Ø¯Ø§Ø¯Ù‡â€ŒÛŒ RAG Ø±Ùˆ ØªØ²Ø±ÛŒÙ‚ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
    - Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø±Ùˆ Ù…ÛŒâ€ŒØ°Ø§Ø±Ù‡
    """

    merged_context = "\n\n".join(
        f"- {chunk.strip()}" for chunk in context_chunks if chunk and chunk.strip()
    )

    # Ø³Ø¨Ú© Ùˆ Ù‡ÙˆÛŒØª Ø¨Ø±Ù†Ø¯ Ø§ÛŒÙ†Ø¬Ø§Ø³Øª ğŸ”¥
    system_instructions = (
        "ØªÙˆ ÛŒÚ© Ù…Ù†ØªÙˆØ± Ø´Ø®ØµÛŒ ÙØ§Ø±Ø³ÛŒâ€ŒØ²Ø¨Ø§Ù† Ù‡Ø³ØªÛŒ Ø¨Ù‡ Ù†Ø§Ù… Â«Ù…Ù†ØªÙˆØ± Ø§Ù…ÛŒÙ†Â». "
        "ØªÙˆ Ø¨Ø§ÛŒØ¯ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ø¹Ù…Ù„â€ŒÚ¯Ø±Ø§ØŒ Ù…Ø­ØªØ±Ù…Ø§Ù†Ù‡ Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ø¨Ø¯ÛŒ. "
        "ØªÙˆØ¶ÛŒØ­Ø§ØªØª Ø¨Ø§ÛŒØ¯ Ú©ÙˆØªØ§Ù‡ØŒ ØµØ±ÛŒØ­ Ùˆ Ù…Ù‡Ø±Ø¨Ø§Ù† Ø¨Ø§Ø´Ù‡. "
        "Ø¯Ø± ØµÙˆØ±Øª Ø§Ù…Ú©Ø§Ù† Ø§Ø² Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ùˆ Ù‚Ø¯Ù…â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†. "
        "Ø§Ú¯Ø± Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø±Ø´Ø¯ ÙØ±Ø¯ÛŒØŒ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ù‡ Ù†ÙØ³ØŒ Ù…Ø°Ø§Ú©Ø±Ù‡ØŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ÛŒØ§ ØªÙˆØ³Ø¹Ù‡ ÙØ±Ø¯ÛŒ Ø§Ø³Øª "
        "Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§ Ø¢Ù…ÙˆØ²Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ø§Ù…ÛŒÙ† Ø§Ø³Øª. "
        "Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ú©Ø§ÙÛŒ Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø² Ø¯Ø§Ù†Ø´ Ø¹Ù…ÙˆÙ…ÛŒ Ø®ÙˆØ¯Øª Ú©Ù…Ú© Ø¨Ú¯ÛŒØ± "
        "ÙˆÙ„ÛŒ Ù„Ø­Ù† Ùˆ Ù‡ÙˆÛŒØª Â«Ù…Ù†ØªÙˆØ± Ø§Ù…ÛŒÙ†Â» Ø±Ø§ Ø­ÙØ¸ Ú©Ù†."
    )

    prompt = f"""
[Ù‡ÙˆÛŒØª Ù…Ù†ØªÙˆØ±]
{system_instructions}

[Ø¯Ø§Ù†Ø´ Ø¯Ø§Ø®Ù„ÛŒ (context)]
{merged_context if merged_context else "(Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø¯Ø§Ø¯Ù‡ Ø¯Ø§Ø®Ù„ÛŒ Ø²ÛŒØ§Ø¯ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª.)"}

[Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±]
{user_question}

[Ø¯Ø³ØªÙˆØ± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®]
Ù¾Ø§Ø³Ø®ÛŒ Ø±ÙˆØ§Ù† Ùˆ Ø­Ù…Ø§ÛŒØªÛŒ Ø¨Ø¯Ù‡. Ø¯Ø± Ø­Ø¯ Ú†Ù†Ø¯ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§ÙØŒ Ù…Ø´Ø®Øµ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ.
Ø§Ø² Ú©Ù„ÛŒâ€ŒÚ¯ÙˆÛŒÛŒÙ Ø¨ÛŒâ€ŒÙØ§ÛŒØ¯Ù‡ Ùˆ Ù†ØµÛŒØ­Øª ØªÚ©Ø±Ø§Ø±ÛŒ Ø¯ÙˆØ±ÛŒ Ú©Ù†.
Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø§Ø³Øª Ù…Ø±Ø­Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ø¨Ø¯Ù‡ (Ù‚Ø¯Ù… Û±ØŒ Ù‚Ø¯Ù… Û² Ùˆ ...).
    """.strip()

    return prompt


def generate_hybrid_answer(
    user_question: str,
    retrieved_docs: List[Dict[str, Any]],
    max_new_tokens: int = 200,
) -> str:
    """
    Ø§ÛŒÙ† Ù‡Ù…ÙˆÙ† Ù‚Ù„Ø¨ Ø­Ø§Ù„Øª Hybrid Ù‡Ø³Øª.
    ÙˆØ±ÙˆØ¯ÛŒ:
      - Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±
      - Ø³Ù†Ø¯Ù‡Ø§ÛŒ retrieve Ø´Ø¯Ù‡ Ø§Ø² Ø¯ÛŒØªØ§Ù‡Ø§ÛŒ Ø§Ø®ØªØµØ§ØµÛŒ (Ù„ÛŒØ³Øª Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ keys Ù…Ø«Ù„ text, source,...)

    Ø®Ø±ÙˆØ¬ÛŒ:
      - Ù¾Ø§Ø³Ø® Ù…Ú©Ø§Ù„Ù…Ù‡â€ŒØ§ÛŒ ÙˆÙ„ÛŒ Ø¢Ú¯Ø§Ù‡ Ø§Ø² Ø¯ÛŒØªØ§ÛŒ Ù…Ø§
    """

    # ÙÙ‚Ø· Ù…Ø­ØªÙˆØ§ Ø±Ùˆ Ø¨Ú©Ø´ÛŒÙ… Ø¨ÛŒØ±ÙˆÙ†
    context_strings = []
    for hit in retrieved_docs:
        # Ù‡Ø± hit Ø´Ú©Ù„ÛŒ Ø´Ø¨ÛŒÙ‡ {"text": "...", "source": "...", ...} Ø¯Ø§Ø±Ù‡
        if "text" in hit and isinstance(hit["text"], str):
            context_strings.append(hit["text"])

    prompt = _build_hybrid_prompt(
        user_question=user_question,
        context_chunks=context_strings
    )

    answer = _call_text_generation_api(prompt, max_new_tokens=max_new_tokens)

    # ÛŒÙ‡ ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ø®ÛŒÙ„ÛŒ Ø³Ø§Ø¯Ù‡:
    # Ú¯Ø§Ù‡ÛŒ HF Ø¨Ø®Ø´ÛŒ Ø§Ø² prompt Ø±Ùˆ echo Ù…ÛŒâ€ŒÚ©Ù†Ù‡ØŒ Ù…Ø§ ÙÙ‚Ø· Ø§Ù†ØªÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø±Ùˆ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒÙ….
    # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø³Ø§Ø¯Ù‡: Ø¢Ø®Ø±ÛŒÙ† Ø­Ø¶ÙˆØ± Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø±Ùˆ Ù¾ÛŒØ¯Ø§ Ú©Ù† Ùˆ Ø¨Ø¹Ø¯Ø´ Ø±Ùˆ Ù†Ú¯Ù‡ Ø¯Ø§Ø±.
    tail_split_marker = "[Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±]"
    if tail_split_marker in answer:
        answer = answer.split(tail_split_marker)[-1].strip()
    return answer.strip()
