#FEYZ
#DEO
import os
from typing import Literal, List, Dict, Any

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from openai import OpenAI

# .env Ø±Ùˆ Ù„ÙˆØ¯ Ú©Ù†
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_CHEAP = os.getenv("OPENAI_MODEL_CHEAP", "gpt-4o-mini")
MODEL_DEEP = os.getenv("OPENAI_MODEL_DEEP", "gpt-4o-mini")

client = OpenAI(api_key=OPENAI_API_KEY)

app = FastAPI()

# Ø§Ø¬Ø§Ø²Ù‡ Ø¯Ø³ØªØ±Ø³ÛŒ ÙØ±Ø§Ù†Øª (Ø­ØªÛŒ ÙˆÙ‚ØªÛŒ Ø¨Ø§ file:// Ø¨Ø§Ø² Ø´Ø¯Ù‡)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Ø­Ø§ÙØ¸Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡Ù” Ø³Ø±ÙˆØ± (ØªØ§ ÙˆÙ‚ØªÛŒ uvicorn Ø±ÙˆØ´Ù†Ù‡)
if not hasattr(app.state, "memory"):
    app.state.memory = []  # list[{"role": "...", "content": "..."}]

class ChatRequest(BaseModel):
    message: str
    length: Literal["short", "normal", "long"] = "short"

class ChatResponse(BaseModel):
    answer: str

def build_length_instruction(length: str) -> str:
    if length == "short":
        return "Ù¾Ø§Ø³Ø® Ø±Ø§ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø¯Ù‡ (Û² ØªØ§ Û³ Ø¬Ù…Ù„Ù‡ Ø®Ù„Ø§ØµÙ‡ Ùˆ Ø§Ø¬Ø±Ø§ÛŒÛŒ)."
    elif length == "long":
        return (
            "Ù¾Ø§Ø³Ø® Ø±Ø§ Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ± Ùˆ Ù…Ø±Ø­Ù„Ù‡â€ŒØ¨Ù‡â€ŒÙ…Ø±Ø­Ù„Ù‡ Ø¨Ø¯Ù‡. Ø¯Ù„ÛŒÙ„ Ù‡Ø± Ù‚Ø¯Ù… Ø±Ø§ Ù‡Ù… ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡. "
            "Ø­Ø¯Ø§Ù‚Ù„ Ûµ-Û¶ Ø¬Ù…Ù„Ù‡ Ø¨Ù†ÙˆÛŒØ³. Ù…Ø«Ø§Ù„ Ù‡Ù… Ø¨Ø²Ù†."
        )
    else:
        return "Ù¾Ø§Ø³Ø® Ø±Ø§ Ø´ÙØ§Ù Ùˆ Ø§Ø¬Ø±Ø§ÛŒÛŒ Ø¨Ø¯Ù‡ Ø¯Ø± Ø­Ø¯ Û³ ØªØ§ Û´ Ø¬Ù…Ù„Ù‡. Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø§Ø´."

@app.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    user_question = req.message.strip()
    style_hint = build_length_instruction(req.length)

    if not user_question:
        return {"answer": "Ø³ÙˆØ§Ù„ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯. ÛŒÚ© Ø³ÙˆØ§Ù„ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ù¾Ø±Ø³ ğŸ™‚"}

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø± Ø¨Ù‡ Ø­Ø§ÙØ¸Ù‡
    app.state.memory.append({
        "role": "user",
        "content": user_question
    })

    # Ù…Ø§ ÙÙ‚Ø· Ø¢Ø®Ø±ÛŒÙ† ~6 Ù¾ÛŒØ§Ù… Ø±Ø§ Ù…ÛŒâ€ŒÙØ±Ø³ØªÛŒÙ… Ø¨Ù‡ Ù…Ø¯Ù„ ØªØ§ Ù‡Ø²ÛŒÙ†Ù‡ Ùˆ Ø·ÙˆÙ„ Ú©Ù†ØªØ±Ù„ Ø´ÙˆØ¯
    recent_dialog: List[Dict[str, Any]] = app.state.memory[-6:]

    # Ù¾ÛŒØ§Ù… system + ØªØ§Ø±ÛŒØ®Ú†Ù‡
    messages_for_model = [
        {
            "role": "system",
            "content": (
                "ØªÙˆ Â«Ù…Ù†ØªÙˆØ± Ø´Ø®ØµÛŒ Ø§Ù…ÛŒÙ†Â» Ù‡Ø³ØªÛŒ. Ø®ÛŒÙ„ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒØŒ ÙˆØ§Ø¶Ø­ Ùˆ Ø¨Ø¯ÙˆÙ† Ø­Ø§Ø´ÛŒÙ‡ Ø¬ÙˆØ§Ø¨ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒ. "
                "ØªÙ… ØªÙ…Ø±Ú©Ø²: Ø¨ÛŒØ²ÛŒÙ†Ø³ØŒ ÙØ±ÙˆØ´ØŒ Ù…Ø°Ø§Ú©Ø±Ù‡ØŒ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ. "
                "Ø¬ÙˆØ§Ø¨ Ø¨Ø§ÛŒØ¯ Ù‚Ø§Ø¨Ù„â€ŒØ§Ø¬Ø±Ø§ Ø¨Ø§Ø´Ø¯. Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ù…Ø¨Ù‡Ù… Ø¨ÙˆØ¯ØŒ Ø§ÙˆÙ„ Ø³ÙˆØ§Ù„ Ø±Ø§ ÙˆØ§Ø¶Ø­ Ú©Ù†. "
                "Ø§Ø² ØªØ¦ÙˆØ±ÛŒ Ø®Ø§Ù„Øµ Ø¨Ø¯ÙˆÙ† Ø¹Ù…Ù„ Ù‚Ø§Ø¨Ù„ Ø§Ù†Ø¬Ø§Ù… Ù¾Ø±Ù‡ÛŒØ² Ú©Ù†."
            ),
        },
        {
            "role": "system",
            "content": (
                f"Ø·ÙˆÙ„ Ù¾Ø§Ø³Ø® Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø± Ú©Ø§Ø±Ø¨Ø±: {req.length}. "
                f"{style_hint}"
            ),
        },
    ] + recent_dialog

    # ØªÙ…Ø§Ø³ Ø¨Ø§ Ù…Ø¯Ù„
    completion = client.chat.completions.create(
        model=MODEL_CHEAP,
        messages=messages_for_model,
        temperature=0.6,
        max_tokens=500,
    )

    raw_answer = ""
    if completion.choices and completion.choices[0].message:
        raw_answer = (completion.choices[0].message.content or "").strip()

    if raw_answer == "":
        raw_answer = (
            "Ø§Ù„Ø§Ù† Ù†ØªÙˆÙ†Ø³ØªÙ… Ø¬ÙˆØ§Ø¨ Ù…Ù†Ø§Ø³Ø¨ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ù…. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ù¾Ø±Ø³ ÛŒØ§ Ù…Ø´Ø®Øµâ€ŒØªØ± Ø¨Ú¯Ùˆ Ø¯Ù‚ÛŒÙ‚Ø§ Ú©Ø¬Ø§ Ú¯ÛŒØ± Ú©Ø±Ø¯ÛŒ."
        )

    # Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„ Ù‡Ù… Ø¨Ù‡ Ø­Ø§ÙØ¸Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
    app.state.memory.append({
        "role": "assistant",
        "content": raw_answer
    })

    return {"answer": raw_answer}
