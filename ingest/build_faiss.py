# ingest/build_faiss.py
# Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø¯Ø§Ø®Ù„ Ù¾ÙˆØ´Ù‡ data/
# Ø®Ø±ÙˆØ¬ÛŒ: D:/Amin_Mentor/faiss_index/index.faiss  +  meta.json

import os
import json
import glob
from pathlib import Path

import numpy as np
import faiss
from sentence_transformers import SentenceTransformer


# ---------- ØªÙ†Ø¸ÛŒÙ… Ù…Ø³ÛŒØ±Ù‡Ø§ ----------
BASE_DIR = Path(__file__).resolve().parents[1]
DATA_DIR = BASE_DIR / "data"
OUT_DIR = BASE_DIR / "faiss_index"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ----------
EMBED_MODEL = os.getenv("EMBED_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
MAX_CHARS = int(os.getenv("CHUNK_MAX_CHARS", "800"))
OVERLAP = int(os.getenv("CHUNK_OVERLAP", "120"))
BATCH_SIZE = int(os.getenv("EMBED_BATCH", "64"))

# Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÙØ§ÛŒÙ„â€Œ ÙˆØ±ÙˆØ¯ÛŒ
FILE_EXTS = ("*.txt", "*.md", "*.srt", "*.vtt")


def chunk_text(text: str, max_chars: int = MAX_CHARS, overlap: int = OVERLAP):
    """
    Ù…ØªÙ† Ø±Ø§ Ø¨Ù‡ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ù‡Ù…Ù¾ÙˆØ´Ø§Ù† ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ø² Ù„ÙˆÙ¾ Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
    """
    text = text.strip().replace("\r", "")
    if max_chars <= 0:
        raise ValueError("max_chars must be > 0")

    if overlap >= max_chars:
        # Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø§Ú¯Ø± Ú©Ø³ÛŒ overlap Ø±Ø§ Ø¨Ø²Ø±Ú¯ Ú¯Ø°Ø§Ø´ØªÙ‡ Ø¨ÙˆØ¯ØŒ Ù…Ù†Ø·Ù‚ÛŒâ€ŒØ§Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        overlap = max_chars // 4

    step = max_chars - overlap
    chunks = []
    i = 0
    n = len(text)

    while i < n:
        end = min(i + max_chars, n)
        chunk = text[i:end].strip()
        if chunk:
            chunks.append(chunk)
        if end == n:
            break  # Ø¨Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒ Ù…ØªÙ† Ø±Ø³ÛŒØ¯ÛŒÙ…
        i += step  # Ø¨Ø§ Ú¯Ø§Ù… Ø«Ø§Ø¨Øª Ø¬Ù„Ùˆ Ø¨Ø±ÙˆÛŒÙ… (Ù†Ù‡ Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ø¹Ù‚Ø¨)

    return chunks


def read_text_file(path: Path) -> str:
    """
    Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ù…Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø¨Ø§ UTF-8 Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡â€ŒÚ¯Ø±ÙØªÙ† Ø®Ø·Ø§Ù‡Ø§ÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±.
    """
    return path.read_text(encoding="utf-8", errors="ignore")


def build_corpus():
    texts, sources = [], []
    # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
    found_any = False
    for ext in FILE_EXTS:
        for fp in glob.glob(str(DATA_DIR / ext)):
            found_any = True
            p = Path(fp)
            raw = read_text_file(p)
            for ch in chunk_text(raw):
                texts.append(ch)
                sources.append({"source": p.name})
    if not found_any:
        raise SystemExit(f"âš ï¸ No input files found in {DATA_DIR}. "
                         f"Put .txt/.md/.srt/.vtt files there and rerun.")
    if not texts:
        raise SystemExit("âš ï¸ Files were found but produced no chunks. "
                         "Check CHUNK_MAX_CHARS/CHUNK_OVERLAP or file encoding.")
    return texts, sources


def main():
    print(f"ğŸ“‚ DATA_DIR: {DATA_DIR}")
    print(f"ğŸ“¦ OUT_DIR : {OUT_DIR}")
    print(f"ğŸ”¤ MODEL   : {EMBED_MODEL}")
    print(f"ğŸ§© chunk   : max={MAX_CHARS}, overlap={OVERLAP}, batch={BATCH_SIZE}")

    texts, sources = build_corpus()

    # Ù…Ø¯Ù„ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯
    model = SentenceTransformer(EMBED_MODEL)

    # Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± Ø¨Ú†â€ŒÙ‡Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†ÛŒÙ… ØªØ§ Ø­Ø§ÙØ¸Ù‡ Ù…Ø¯ÛŒØ±ÛŒØª Ø´ÙˆØ¯
    embs_list = []
    total = len(texts)
    for start in range(0, total, BATCH_SIZE):
        end = min(start + BATCH_SIZE, total)
        batch = texts[start:end]
        emb = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)
        embs_list.append(emb)
        print(f"â€¦ embedded {end}/{total}")

    embs = np.vstack(embs_list).astype(np.float32)

    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ cosine (dot-product Ø¨Ø§ IndexFlatIP)
    norms = np.linalg.norm(embs, axis=1, keepdims=True) + 1e-10
    embs = embs / norms

    # Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ùˆ Ø§ÙØ²ÙˆØ¯Ù†
    index = faiss.IndexFlatIP(embs.shape[1])
    index.add(embs)

    # Ø°Ø®ÛŒØ±Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ùˆ Ù…ØªØ§
    index_path = OUT_DIR / "index.faiss"
    meta_path = OUT_DIR / "meta.json"

    faiss.write_index(index, str(index_path))
    meta_path.write_text(
        json.dumps({"texts": texts, "sources": sources}, ensure_ascii=False),
        encoding="utf-8",
    )

    print(f"âœ… Indexed {len(texts)} chunks â†’ {index_path}")
    print(f"ğŸ“ Meta saved â†’ {meta_path}")


if __name__ == "__main__":
    main()
