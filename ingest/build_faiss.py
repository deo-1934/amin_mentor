"""
build_faiss.py
---------------
Ø§ÛŒØ¬Ø§Ø¯ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨Ø§ FAISS Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡.

Ø´Ø±Ø­ Ø¹Ù…Ù„Ú©Ø±Ø¯:
- Ù…ØªÙ† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…Ø³ÛŒØ± ./data Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯.
- Ù‡Ø± Ù…ØªÙ† Ø±Ø§ Ø¨Ù‡ Ù‚Ø·Ø¹Ø§Øª Ú©ÙˆÚ†Ú©â€ŒØªØ± (chunk) ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù‚Ø·Ø¹Ù‡ØŒ embedding Ù…Ø¹Ù†Ø§ÛŒÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²Ù‡Ø§:
    pip install faiss-cpu sentence-transformers numpy

Ø®Ø±ÙˆØ¬ÛŒ:
    ./faiss_index/index.faiss
    ./faiss_index/store.pkl
"""

from __future__ import annotations
import os
import uuid
import pickle
import glob
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer


# Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡
ROOT_DIR = Path(__file__).resolve().parents[1]
DATA_DIR = ROOT_DIR / "data"
INDEX_DIR = ROOT_DIR / "faiss_index"
INDEX_DIR.mkdir(parents=True, exist_ok=True)

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"  # 384 Ø¨Ø¹Ø¯
CHUNK_SIZE = 800
CHUNK_OVERLAP = 100


def chunk_text(text: str, size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    """ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ù‚Ø·Ø¹Ø§Øª Ø¨Ø§ Ø·ÙˆÙ„ Ø«Ø§Ø¨Øª."""
    text = " ".join(text.split())
    chunks, i = [], 0
    while i < len(text):
        chunks.append(text[i:i + size])
        i += max(1, size - overlap)
    return [c for c in chunks if c.strip()]


def load_text_files() -> Tuple[List[str], List[Dict]]:
    """Ø®ÙˆØ§Ù†Ø¯Ù† ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø§Ø² Ù…Ø³ÛŒØ± data Ùˆ ØªÙ‚Ø³ÛŒÙ… Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ù‡ Ù‚Ø·Ø¹Ø§Øª."""
    files = sorted(glob.glob(str(DATA_DIR / "*.txt")))
    if not files:
        raise FileNotFoundError(f"Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø¯Ø± Ù…Ø³ÛŒØ± {DATA_DIR} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
    documents, metadata = [], []
    for path in files:
        text = Path(path).read_text(encoding="utf-8", errors="ignore")
        chunks = chunk_text(text)
        for i, ch in enumerate(chunks):
            documents.append(ch)
            metadata.append({"id": str(uuid.uuid4()), "source": os.path.basename(path), "chunk_idx": i})
    return documents, metadata


def build_faiss_index(vectors: np.ndarray) -> faiss.Index:
    """Ø§ÛŒØ¬Ø§Ø¯ Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§."""
    dim = vectors.shape[1]
    index = faiss.IndexHNSWFlat(dim, 32)
    index.hnsw.efConstruction = 200
    index.add(vectors)
    return index


def main() -> None:
    """Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³."""
    print(f"ğŸ“‚ Ù…Ø³ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {DATA_DIR}")
    documents, metadata = load_text_files()
    print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù‚Ø·Ø¹Ø§Øª: {len(documents)}")

    # ØªÙˆÙ„ÛŒØ¯ embedding
    print("ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒÛŒ embedding...")
    model = SentenceTransformer(MODEL_NAME)
    embeddings = model.encode(documents, normalize_embeddings=True, batch_size=64, show_progress_bar=True)
    embeddings = np.asarray(embeddings, dtype="float32")

    # Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³
    print("âš™ï¸ Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS...")
    index = build_faiss_index(embeddings)

    # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
    faiss.write_index(index, str(INDEX_DIR / "index.faiss"))
    with open(INDEX_DIR / "store.pkl", "wb") as f:
        pickle.dump({"docs": documents, "meta": metadata, "model": MODEL_NAME}, f)

    print("ğŸ¯ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø³Ø§Ø®ØªÙ‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    print(f"ğŸ“¦ Ù…Ø³ÛŒØ± Ø§ÛŒÙ†Ø¯Ú©Ø³: {INDEX_DIR / 'index.faiss'}")
    print(f"ğŸ—‚ï¸ Ù…Ø³ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {INDEX_DIR / 'store.pkl'}")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§: {e}")
